[
["index.html", "Homework2 Chapter 1 Exercise 1.2 1.1 Use Monte Carlo to estimate \\(\\Phi(t)\\) 1.2 Boxplots of bias", " Homework2 Jieying Jiao 2018-09-12 Chapter 1 Exercise 1.2 1.1 Use Monte Carlo to estimate \\(\\Phi(t)\\) The Monte Carlo methods gives: \\[\\hat{\\Phi}(t) = \\frac{1}{n}\\sum_{i = 1}^nI(X_i\\leqslant t)\\] where \\(X_i\\) are random normal sample drawn from \\(N(0, 1)\\). library(xtable) phi.MC &lt;- function(n, t) { phi.hat &lt;- matrix(0, nrow = length(n), ncol = length(t)) for (i in 1:length(n)) { for (j in 1:length(t)) { X &lt;- rnorm(n[i], 0, 1) phi.hat[i, j] &lt;- mean(as.numeric(X &lt;= t[j])) } } truth &lt;- pnorm(t) phi.hat &lt;- rbind(phi.hat, truth) } n &lt;- c(10^2, 10^3, 10^4) t &lt;- c(0.0, 0.67, 0.84, 1.28, 1.65, 2.32, 2.58, 3.09, 3.72) set.seed(1) phi.hat &lt;- phi.MC(n, t) rownames(phi.hat) &lt;- c(paste0(&quot;n = 10e&quot;, 2:4), &quot;truth&quot;) colnames(phi.hat) &lt;- paste0(&quot;t = &quot;, t) t.MC &lt;- xtable(phi.hat, digits = 4, caption = &quot;Monte Carlo estimation&quot;, label = &quot;MC result&quot;) The estimation results are shown in Table . 1.2 Boxplots of bias Repeat the above expriment 100 times and plot bias of Monte Carlo methods at every time point \\(t\\) using boxplots. library(ggplot2) nsim &lt;- 100 bias &lt;- data.frame(bias = rep(0, nsim * length(t) * length(n)), t = rep(t, length(n) * nsim), n = rep(rep(n, each = length(t)), nsim)) for (i in 1:nsim) { phi.hat &lt;- phi.MC(n, t) bias$bias[((i-1)*length(t)*length(n)+1):(i*length(t)*length(n))] &lt;- as.vector(t(phi.hat[1:length(n), ]) - phi.hat[length(n) + 1, ]) } bias$t &lt;- as.factor(bias$t) ggplot(data = bias, aes(x = t, y = bias)) + geom_boxplot(outlier.colour=&quot;red&quot;, outlier.shape=8, outlier.size=4) + facet_wrap( ~ n) + theme(text = element_text(size = 10)) "],
["exercise-1-3.html", "Chapter 2 Exercise 1.3", " Chapter 2 Exercise 1.3 .Machine$double.xmax ## [1] 1.797693e+308 .Machine$double.xmin ## [1] 2.225074e-308 .Machine$double.eps ## [1] 2.220446e-16 .Machine$double.neg.eps ## [1] 1.110223e-16 Floating number is represented in computer as : \\[(-1)^{x_0}(1+\\sum_{i = 1}^tx_i2^{-i})2^k\\] In a 64 bits machine, \\(x_0\\) takes 1 sign bites, significant takes 52 bits, so \\(t\\) can be 52 at most. Exponent \\(k\\) takes 11 bits, so \\(2^11 = 2048\\) possible values. With shifting to negative side, k can be from -1022 to 1024, with one left for sign. “.Machine$double.xmax” is the largest floating number that computer can display, it is \\((1+\\sum_{i = 1}^{52}2^{-i})\\times 2^{1024}\\) “.Machine$double.xmin” is the smallest floating numer that computer can display, it is \\(2^{-1022}\\). “.Machine$double.eps” is the smallest positive floating number that the computer can tell the difference by adding it. It is actually the smallest significant, that is \\(2^{-52}\\). “.Machine$double.neg.eps” is the smallest positive floating number that the computer can tell the difference by substracting it. It is \\(2^{-53}\\). "],
["references.html", "References", " References "]
]
